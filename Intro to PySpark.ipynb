{"cells":[{"cell_type":"markdown","source":["# Welcome to a PySpark notebook in Microsoft Fabric!\n","---\n","### This is an example of a *Markdown* cell\n","##### Before we look at some code, let's review some aspects of the UI\n","1. Attaching lakehouse(s) to the notebook (see below code snippet for environment info)\n","2. Connect to Session\n","3. Run all button\n","4. Setting the Default language for the notebook \n","\n","### Hey DataBard! Don't forget to tell them about Data Wrangler!"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"c898d845-9bff-406c-8b4e-1ae9f8ee3e29"},{"cell_type":"code","source":["%%pyspark \n","!echo \"spark.trident.pbiApiVersion=v1\">>/home/trusted-service-user/.trident-context\n","#Word to the wise! If you are using Fabric Lakehouse schemas in preview, there is a known bug that returns a 'forbidden' error when interacting with the lakehouse.\n","#If you run into this, use this code to work around it!"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"4108c2f8-7ff4-4e82-8af3-3e289164ccc5"},{"cell_type":"code","source":["#Parameter cell - These variables can be populated by external solutions, like pipelines.\n","\n","Parameter1 = ''"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"},"tags":["parameters"]},"id":"e6686888-97d5-4971-8962-32d5a3d6fb7b"},{"cell_type":"code","source":["#What if I want details about my environment?\n","\n","#From Bradley Schacht, 'Gathering useful notebook and environment details at runtime'\n","#%pip install semantic-link\n","\n","#import trident\n","\n","default_lakehouse_id    = 'No default lakehouse' if spark.conf.get(\"trident.lakehouse.id\") == '' else spark.conf.get(\"trident.lakehouse.id\")\n","default_lakehouse_name  = 'No default lakehouse' if spark.conf.get(\"trident.lakehouse.name\") == '' else spark.conf.get(\"trident.lakehouse.name\")\n","notebook_item_id        = spark.conf.get(\"trident.artifact.id\")\n","#notebook_item_name      = spark.conf.get(\"trident.artifact.name\")\n","pool_executor_cores     = spark.sparkContext.getConf().get(\"spark.executor.cores\")\n","pool_executor_memory    = spark.sparkContext.getConf().get(\"spark.executor.memory\")\n","pool_min_executors      = spark.sparkContext.getConf().get(\"spark.dynamicAllocation.minExecutors\")\n","pool_max_executors      = spark.sparkContext.getConf().get(\"spark.dynamicAllocation.maxExecutors\")\n","pool_number_of_nodes    = len(str(sc._jsc.sc().getExecutorMemoryStatus().keys()).replace(\"Set(\",\"\").replace(\")\",\"\").split(\", \"))\n","spark_app_name          = spark.sparkContext.getConf().get(\"spark.app.name\")[::-1].split(\"_\",1)[0][::-1]\n","workspace_id            = spark.conf.get(\"trident.artifact.workspace.id\")\n","#workspace_name          = spark.conf.get(\"trident.artifact.workspace.name\")\n","\n","print(f'default_lakehouse_id:   {default_lakehouse_id}')\n","print(f'default_lakehouse_name: {default_lakehouse_name}')\n","print(f'notebook_item_id:       {notebook_item_id}')\n","#print(f'notebook_item_name:     {notebook_item_name}')\n","print(f'spark_app_name:         {spark_app_name}')\n","print(f'pool_executor_cores:    {pool_executor_cores}')\n","print(f'pool_executor_memory:   {pool_executor_memory}')\n","print(f'pool_min_executors:     {pool_min_executors}')\n","print(f'pool_max_executors:     {pool_max_executors}')\n","print(f'pool_number_of_nodes:   {pool_number_of_nodes}')\n","print(f'workspace_id:           {workspace_id}')\n","#print(f'workspace_name:         {workspace_name}')\n","\n","#Did you attach a default lakehouse?\n","\n","#What about accessing Lakehouses outside of the default? Copy Path!\n"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":3,"statement_ids":[3],"state":"finished","livy_statement_state":"available","session_id":"ba4c6832-b450-4e8a-a951-ef3cce2fcfbc","normalized_state":"finished","queued_time":"2024-09-07T19:28:24.4522736Z","session_start_time":null,"execution_start_time":"2024-09-07T19:28:29.9091898Z","execution_finish_time":"2024-09-07T19:28:32.9997425Z","parent_msg_id":"bc0f2cab-c7cf-49bf-8b8f-588c4cd48129"},"text/plain":"StatementMeta(, ba4c6832-b450-4e8a-a951-ef3cce2fcfbc, 3, Finished, Available, Finished)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["default_lakehouse_id:   75de1ae4-93f7-4582-9799-ca34d6429709\ndefault_lakehouse_name: Databard_Demo\nnotebook_item_id:       2dd1eb8c-34cd-491f-9575-850a064f397f\nspark_app_name:         ba4c6832-b450-4e8a-a951-ef3cce2fcfbc\npool_executor_cores:    8\npool_executor_memory:   56g\npool_min_executors:     1\npool_max_executors:     1\npool_number_of_nodes:   2\nworkspace_id:           e35b5629-0c71-450d-b52f-4e2ecc6836c7\n"]}],"execution_count":1,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"f3d855a3-3504-4ba6-a02f-69808ee2a829"},{"cell_type":"code","source":["#Dataframes: Choose carefully\n","import pandas as pd\n","\n","# Create a pandas dataframe\n","pandas_df = pd.DataFrame({'Name': ['Alice', 'Bob', 'Charlie'], 'Age': [25, 30, 35], 'Gender': ['F', 'M', 'M']})\n","\n","# Convert pandas dataframe to Spark dataframe\n","spark_df = spark.createDataFrame(pandas_df)\n","\n","#Functionally the dataframes will look the same.\n","#However, the Pandas dataframe will not be able to make full use of the resources available.\n","print(\"Pandas DataFrame:\")\n","print(pandas_df)\n","\n","print(\"\\nSpark DataFrame:\")\n","spark_df.show()\n"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":5,"statement_ids":[5],"state":"finished","livy_statement_state":"available","session_id":"ba4c6832-b450-4e8a-a951-ef3cce2fcfbc","normalized_state":"finished","queued_time":"2024-09-07T19:31:17.4161368Z","session_start_time":null,"execution_start_time":"2024-09-07T19:31:18.0413978Z","execution_finish_time":"2024-09-07T19:31:19.6448941Z","parent_msg_id":"fd3850e3-d272-4633-88c9-e210e1f9530f"},"text/plain":"StatementMeta(, ba4c6832-b450-4e8a-a951-ef3cce2fcfbc, 5, Finished, Available, Finished)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Pandas DataFrame:\n      Name  Age Gender\n0    Alice   25      F\n1      Bob   30      M\n2  Charlie   35      M\n\nSpark DataFrame:\n+-------+---+------+\n|   Name|Age|Gender|\n+-------+---+------+\n|  Alice| 25|     F|\n|    Bob| 30|     M|\n|Charlie| 35|     M|\n+-------+---+------+\n\n"]}],"execution_count":3,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"192c99ad-c875-4460-ad53-412dd80aa803"},{"cell_type":"code","source":["# We know our enviroment. Let's Transform Data!\n","\n","from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n","from pyspark.sql.functions import lit, when\n","import pandas as pd\n","import resource #Used for assessing resources used\n","import random\n","from pyspark.context import SparkContext\n","\n","# # Function to generate data for the dataframe\n","def generate_data(num_rows):\n","    names = ['Alice', 'Bob', 'Charlie']\n","    ages = [25, 30, 35]\n","    occupations = ['Engineer', 'Analyst', 'Manager']\n","    \n","    data = {'Name': ['Alice'], 'Age': [25], 'Occupation': ['Engineer']}  # Fix: Change 'FirstName' to 'Name'\n","    \n","    for _ in range(num_rows):\n","        data['Name'].append(random.choice(names))  # Fix: Change 'FirstName' to 'Name'\n","        data['Age'].append(random.choice(ages))\n","        data['Occupation'].append(random.choice(occupations))\n","    \n","    return spark.createDataFrame(data, ['Name', 'Age', 'Occupation'])\n","\n","\n","# Example: Create a simple dataframe with sample data\n","# Here, we create a DataFrame named df using the createDataFrame method provided by the SparkSession. \n","# The DataFrame is a distributed collection of data organized into named columns. \n","# In this case, we pass a list of tuples containing the sample data and specify the column names as 'Name' and 'Age'.\n","data = [('Alice', 25, 'Manager'), ('Bob', 30, 'Analytst'), ('Charlie', 35, 'Engineer')]\n","df = spark.createDataFrame(data, ['Name', 'Age', 'Occupation'])\n","\n","# new_data = generate_data(100000)\n","# df = df.union(new_data)\n","\n","# Print the initial dataframe to the console in a tabular format\n","print(\"Initial DataFrame:\")\n","df.show()\n","\n","# Display the column names and data types\n","print(\"Data Types:\")\n","df.printSchema()\n","\n","# Step 1: Profile the data - Calculate the average age\n","# The selectExpr() method is used to select and compute an expression on the DataFrame. \n","# In this case, we calculate the average of the 'Age' column and alias it as 'avg_age'. \n","# The collect() method is then used to retrieve the result as a list, and we access the average age value using indexing.\n","average_age = df.selectExpr('avg(Age) as avg_age').collect()[0]['avg_age']\n","print(\"Average Age:\", average_age)\n","\n","# Step 2: Transform the data - Add a new column 'Category' using a case statement\n","# In this step, we add another new column named 'Category' to the DataFrame using a case statement. \n","# The when() function is used to define the conditions and corresponding values for the 'Category' column. \n","# If the 'Age' is less than 30, the value is set to 'Young'. \n","# If the 'Age' is greater than or equal to 30, the value is set to 'Adult'.\n","# Otherwise, the value is set to 'Unknown'. Finally, we print the DataFrame with the new column.\n","df = df.withColumn('Category', when(df['Age'] < 30, 'Young')\n","                               .when(df['Age'] >= 30, 'Adult')\n","                               .otherwise('Unknown'))\n","print(\"\\nDataFrame with Category column:\")\n","df.show()\n","\n","# Step 3: Profile the data - Count the number of records by occupation\n","#Here, we perform a profiling operation on the DataFrame by counting the number of records for each occupation. \n","# The groupBy() method is used to group the DataFrame by the 'Occupation' column, and the count() method is applied to calculate the count for each group. \n","# The result is stored in the occupation_counts DataFrame, and we print it to display the occupation counts.\n","occupation_counts = df.groupBy('Occupation').count()\n","print(\"\\nOccupation Counts:\")\n","occupation_counts.show()\n","\n","# Step 4: Transform the data - Filter records based on age\n","# In this step, we filter the DataFrame to include only the records where the age is greater than 30. \n","# The filter() method is used to apply the filtering condition, and the resulting DataFrame is stored in filtered_df. \n","filtered_df = df.filter(df['Age'] > 30)\n","print(\"\\nFiltered DataFrame:\")\n","filtered_df.show()\n","\n","# Step 5: Profile the data - Describe the dataframe\n","# The describe() method computes summary statistics for each numerical column in the DataFrame, including count, mean, standard deviation, minimum, and maximum values. \n","# The show() method is then used to display the summary statistics, including count, mean, standard deviation, minimum, and maximum values.\n","print(\"\\nDataFrame Description:\")\n","df.describe().show()\n","\n","# Step 6: Display the dataframe\n","# Finally, we display the DataFrame with the new columns and transformations applied.\n","# The display() function is a utility provided by PySpark to render the DataFrame in a way that is suitable for Jupyter notebooks or other frontends. \n","# It allows for more advanced visualizations and interactivity compared to the regular show() method.\n","display(df)\n","\n","# Bonus Content: Create a second dataframe with explicit schema\n","# In this part, we create a second DataFrame named df_explicit_schema with an explicit schema. \n","# The schema is defined using the StructType and StructField classes from the pyspark.sql.types module. \n","# The schema specifies the column names, data types, and nullability constraints. \n","# We then create the DataFrame using the createDataFrame() method, passing the sample data and the explicit schema. Finally, we print the second DataFrame with the explicit schema.\n","schema = StructType([\n","    StructField('Name', StringType(), nullable=False),\n","    StructField('Age', IntegerType(), nullable=False),\n","    StructField('Occupation', StringType(), nullable=False)\n","])\n","df_explicit_schema = spark.createDataFrame(data, schema)\n","\n","print(\"\\nSecond DataFrame with Explicit Schema:\")\n","df_explicit_schema.show()\n","\n","\n","# Define the target directory for the Delta table\n","target_directory = \"abfss://e35b5629-0c71-450d-b52f-4e2ecc6836c7@onelake.dfs.fabric.microsoft.com/75de1ae4-93f7-4582-9799-ca34d6429709/Tables/Employees\"\n","\n","# Save the DataFrame to the Delta table with overwrite mode\n","df.write.format(\"delta\").mode(\"overwrite\").save(target_directory)\n","\n"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":5,"statement_ids":[5],"state":"finished","livy_statement_state":"available","session_id":"a09bd011-5b66-4adc-89bb-6da57a6b2b2b","normalized_state":"finished","queued_time":"2024-09-10T21:39:25.6096902Z","session_start_time":null,"execution_start_time":"2024-09-10T21:39:26.0701741Z","execution_finish_time":"2024-09-10T21:39:34.5706495Z","parent_msg_id":"4df5f34b-a836-401d-9256-85f8bfd287d1"},"text/plain":"StatementMeta(, a09bd011-5b66-4adc-89bb-6da57a6b2b2b, 5, Finished, Available, Finished)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Initial DataFrame:\n+-------+---+----------+\n|   Name|Age|Occupation|\n+-------+---+----------+\n|  Alice| 25|   Manager|\n|    Bob| 30|  Analytst|\n|Charlie| 35|  Engineer|\n+-------+---+----------+\n\nData Types:\nroot\n |-- Name: string (nullable = true)\n |-- Age: long (nullable = true)\n |-- Occupation: string (nullable = true)\n\nAverage Age: 30.0\n\nDataFrame with Category column:\n+-------+---+----------+--------+\n|   Name|Age|Occupation|Category|\n+-------+---+----------+--------+\n|  Alice| 25|   Manager|   Young|\n|    Bob| 30|  Analytst|   Adult|\n|Charlie| 35|  Engineer|   Adult|\n+-------+---+----------+--------+\n\n\nOccupation Counts:\n+----------+-----+\n|Occupation|count|\n+----------+-----+\n|   Manager|    1|\n|  Analytst|    1|\n|  Engineer|    1|\n+----------+-----+\n\n\nFiltered DataFrame:\n+-------+---+----------+--------+\n|   Name|Age|Occupation|Category|\n+-------+---+----------+--------+\n|Charlie| 35|  Engineer|   Adult|\n+-------+---+----------+--------+\n\n\nDataFrame Description:\n+-------+-------+----+----------+--------+\n|summary|   Name| Age|Occupation|Category|\n+-------+-------+----+----------+--------+\n|  count|      3|   3|         3|       3|\n|   mean|   null|30.0|      null|    null|\n| stddev|   null| 5.0|      null|    null|\n|    min|  Alice|  25|  Analytst|   Adult|\n|    max|Charlie|  35|   Manager|   Young|\n+-------+-------+----+----------+--------+\n\n"]},{"output_type":"display_data","data":{"application/vnd.synapse.widget-view+json":{"widget_id":"3e567ea9-02c2-4df7-9ef1-003883f8f112","widget_type":"Synapse.DataFrame"},"text/plain":"SynapseWidget(Synapse.DataFrame, 3e567ea9-02c2-4df7-9ef1-003883f8f112)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["\nSecond DataFrame with Explicit Schema:\n+-------+---+----------+\n|   Name|Age|Occupation|\n+-------+---+----------+\n|  Alice| 25|   Manager|\n|    Bob| 30|  Analytst|\n|Charlie| 35|  Engineer|\n+-------+---+----------+\n\n"]}],"execution_count":2,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"},"collapsed":false},"id":"18aa8ce8-cdbd-4a72-86e9-0050489b6c71"},{"cell_type":"code","source":["# Code generated by Data Wrangler for PySpark DataFrame\n","\n","from pyspark.sql import functions as F\n","\n","def clean_data(df):\n","    # Convert text to lowercase in column: 'Category'\n","    df = df.withColumn('Category', F.lower(F.col('Category')))\n","    # Remove leading and trailing whitespace in column: 'Name'\n","    df = df.withColumn('Name', F.trim(df['Name']))\n","    return df\n","\n","df_clean = clean_data(df)\n","display(df_clean)"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":15,"statement_ids":[15],"state":"finished","livy_statement_state":"available","session_id":"440f11db-3765-4f15-b5b2-bf09498a816e","normalized_state":"finished","queued_time":"2024-09-09T17:52:08.4970314Z","session_start_time":null,"execution_start_time":"2024-09-09T17:52:09.083873Z","execution_finish_time":"2024-09-09T17:52:09.9053195Z","parent_msg_id":"e2b03f6e-ab6c-4527-b652-38f382440146"},"text/plain":"StatementMeta(, 440f11db-3765-4f15-b5b2-bf09498a816e, 15, Finished, Available, Finished)"},"metadata":{}},{"output_type":"display_data","data":{"application/vnd.synapse.widget-view+json":{"widget_id":"327cd94b-2dfb-4cfd-bf78-5b9d722003f4","widget_type":"Synapse.DataFrame"},"text/plain":"SynapseWidget(Synapse.DataFrame, 327cd94b-2dfb-4cfd-bf78-5b9d722003f4)"},"metadata":{}}],"execution_count":4,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"},"collapsed":false},"id":"81a07486-dda6-4c60-b70f-7b9b00589bdc"},{"cell_type":"code","source":["df = spark.sql(\"SELECT * FROM Databard_Demo.Employees LIMIT 1000\")\n","display(df)"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":6,"statement_ids":[6],"state":"finished","livy_statement_state":"available","session_id":"fc309b34-9daf-4abf-a90d-00b12b01aad9","normalized_state":"finished","queued_time":"2024-09-10T22:53:23.6591385Z","session_start_time":null,"execution_start_time":"2024-09-10T22:53:24.1962764Z","execution_finish_time":"2024-09-10T22:53:29.3381719Z","parent_msg_id":"e52c3f5d-6121-474a-a924-cfc0886c5659"},"text/plain":"StatementMeta(, fc309b34-9daf-4abf-a90d-00b12b01aad9, 6, Finished, Available, Finished)"},"metadata":{}},{"output_type":"display_data","data":{"application/vnd.synapse.widget-view+json":{"widget_id":"d566e254-6285-479f-af09-a88ff6cd1a13","widget_type":"Synapse.DataFrame"},"text/plain":"SynapseWidget(Synapse.DataFrame, d566e254-6285-479f-af09-a88ff6cd1a13)"},"metadata":{}}],"execution_count":2,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"},"collapsed":false},"id":"b47b87c2-396f-4567-a98b-379c23a3cb50"},{"cell_type":"code","source":["# Code generated by Data Wrangler for pandas sample\n","\n","import pandas as pd\n","\n","def clean_data(pandas_df):\n","    # Convert text to lowercase in column: 'FirstName'\n","    pandas_df['FirstName'] = pandas_df['FirstName'].str.lower()\n","    # Split text using string '@' in column: 'EmailAddress'\n","    loc_0 = pandas_df.columns.get_loc('EmailAddress')\n","    pandas_df_split = pandas_df['EmailAddress'].str.split(pat='@', expand=True, n=1).add_prefix('EmailAddress_')\n","    pandas_df = pd.concat([pandas_df.iloc[:, :loc_0], pandas_df_split, pandas_df.iloc[:, loc_0:]], axis=1)\n","    pandas_df = pandas_df.drop(columns=['EmailAddress'])\n","    return pandas_df\n","\n","# Loaded variable 'df' from kernel state\n","pandas_df = df.limit(5000).toPandas()\n","\n","pandas_df_clean = clean_data(pandas_df.copy())\n","pandas_df_clean.head()"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"51384d36-6202-4371-98dd-d06c35dbda4f"},{"cell_type":"code","source":["# Code generated by Data Wrangler for PySpark DataFrame\n","\n","from pyspark.sql import functions as F\n","\n","def clean_data(df):\n","    # Convert text to lowercase in column: 'FirstName'\n","    df = df.withColumn('FirstName', F.lower(F.col('FirstName')))\n","    # Split text using string '@' in column: 'EmailAddress'\n","    split_col = F.split(df['EmailAddress'], '@', limit=2)\n","    max_size = df.select(F.max(F.size(split_col))).collect()[0][0]\n","    old_cols = df.columns\n","    new_cols = []\n","    loc_0 = df.columns.index('EmailAddress')\n","    for i in range(max_size):\n","        cur_col_name = 'EmailAddress_%d' % i\n","        new_cols.append(cur_col_name)\n","        df = df.withColumn(cur_col_name, split_col.getItem(i))\n","    df = df.select(*old_cols[:loc_0], *new_cols, *old_cols[loc_0+1:])\n","    return df\n","\n","df_clean = clean_data(df)\n","display(df_clean)"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"6bc801d1-dcde-4c52-8e7a-a43d71f56344"},{"cell_type":"markdown","source":["## Formatting Text in Markdown\n","\n","Markdown provides several options for formatting text in a Jupyter Notebook markdown cell. These options include:\n","\n","### 1. Headers\n","\n","Headers are used to create different levels of headings. There are six levels of headers available in markdown, denoted by the number of hash symbols (#) used before the text. For example:\n","\n","```\n","# Heading 1\n","## Heading 2\n","### Heading 3\n","#### Heading 4\n","##### Heading 5\n","###### Heading 6\n","```\n","\n","### 2. Emphasis\n","\n","To emphasize text, you can use asterisks (*) or underscores (_) around the text. Here are some examples:\n","\n","```\n","*Italic text*\n","_Italic text_\n","\n","**Bold text**\n","__Bold text__\n","\n","***Bold and italic text***\n","___Bold and italic text___\n","```\n","\n","### 3. Lists\n","\n","Markdown supports both ordered and unordered lists. For unordered lists, you can use asterisks (*), plus signs (+), or hyphens (-) as bullet points. For example:\n","\n","```\n","- Item 1\n","- Item 2\n","- Item 3\n","\n","* Item 1\n","* Item 2\n","* Item 3\n","\n","+ Item 1\n","+ Item 2\n","+ Item 3\n","```\n","\n","For ordered lists, you can use numbers followed by periods. For example:\n","\n","```\n","1. Item 1\n","2. Item 2\n","3. Item 3\n","```\n","\n","### 4. Links\n","\n","To create a hyperlink, you can use square brackets [] to enclose the link text, followed by parentheses () containing the URL. For example:\n","\n","```\n","[GitHub](https://github.com)\n","```\n","\n","### 5. Images\n","\n","To display an image, you can use an exclamation mark (!), followed by square brackets [] containing the alt text, and parentheses () containing the image URL. For example:\n","\n","```\n","![Alt Text](https://example.com/image.jpg)\n","```\n","\n","### 6. Code Blocks\n","\n","To display code blocks, you can use triple backticks (```) before and after the code. You can also specify the programming language for syntax highlighting. For example:\n","\n","\\```python\n","print(\"Hello, World!\")\n","\\```\n","\n","### 7. Horizontal Lines\n","\n","To insert a horizontal line, you can use three or more hyphens (-), asterisks (*), or underscores (_). For example:\n","\n","```\n","---\n","```\n","\n","These are just some of the formatting options available in markdown. You can explore more advanced features and syntax by referring to the markdown documentation.\n","\n","Feel free to experiment with these formatting options in your Jupyter Notebook markdown cells to create visually appealing and well-structured documentation for your workflow."],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"415ff63e-1192-4dba-8db4-c9fe8924687d"}],"metadata":{"kernel_info":{"name":"synapse_pyspark"},"kernelspec":{"name":"synapse_pyspark","language":"Python","display_name":"Synapse PySpark"},"language_info":{"name":"python"},"microsoft":{"language":"python","language_group":"synapse_pyspark","ms_spell_check":{"ms_spell_check_language":"en"}},"widgets":{},"nteract":{"version":"nteract-front-end@1.0.0"},"synapse_widget":{"version":"0.1","state":{"327cd94b-2dfb-4cfd-bf78-5b9d722003f4":{"type":"Synapse.DataFrame","sync_state":{"table":{"rows":[{"0":"Alice","1":"25","2":"Manager","3":"young"},{"0":"Bob","1":"30","2":"Analytst","3":"adult"},{"0":"Charlie","1":"35","2":"Engineer","3":"adult"}],"schema":[{"key":"0","name":"Name","type":"string"},{"key":"1","name":"Age","type":"bigint"},{"key":"2","name":"Occupation","type":"string"},{"key":"3","name":"Category","type":"string"}],"truncated":false},"isSummary":false,"language":"scala","wranglerEntryContext":{"candidateVariableNames":["df_clean"],"dataframeType":"pyspark"}},"persist_state":{"view":{"type":"details","tableOptions":{},"chartOptions":{"chartType":"bar","categoryFieldKeys":["0"],"seriesFieldKeys":["0"],"aggregationType":"count","isStacked":false,"binsNumber":10,"wordFrequency":"-1","evaluatesOverAllRecords":false}}}},"3e567ea9-02c2-4df7-9ef1-003883f8f112":{"type":"Synapse.DataFrame","sync_state":{"table":{"rows":[{"0":"Alice","1":"25","2":"Manager","3":"Young"},{"0":"Bob","1":"30","2":"Analytst","3":"Adult"},{"0":"Charlie","1":"35","2":"Engineer","3":"Adult"}],"schema":[{"key":"0","name":"Name","type":"string"},{"key":"1","name":"Age","type":"bigint"},{"key":"2","name":"Occupation","type":"string"},{"key":"3","name":"Category","type":"string"}],"truncated":false},"isSummary":false,"language":"scala","wranglerEntryContext":{"candidateVariableNames":["df"],"dataframeType":"pyspark"}},"persist_state":{"view":{"type":"details","tableOptions":{},"chartOptions":{"chartType":"bar","categoryFieldKeys":["0"],"seriesFieldKeys":["0"],"aggregationType":"count","isStacked":false,"binsNumber":10,"wordFrequency":"-1","evaluatesOverAllRecords":false}}}},"d566e254-6285-479f-af09-a88ff6cd1a13":{"type":"Synapse.DataFrame","sync_state":{"table":{"rows":[{"0":"Alice","1":"25","2":"Manager","3":"Young"},{"0":"Bob","1":"30","2":"Analytst","3":"Adult"},{"0":"Charlie","1":"35","2":"Engineer","3":"Adult"}],"schema":[{"key":"0","name":"Name","type":"string"},{"key":"1","name":"Age","type":"bigint"},{"key":"2","name":"Occupation","type":"string"},{"key":"3","name":"Category","type":"string"}],"truncated":false},"isSummary":false,"language":"scala","wranglerEntryContext":{"candidateVariableNames":["df"],"dataframeType":"pyspark"}},"persist_state":{"view":{"type":"details","tableOptions":{},"chartOptions":{"chartType":"bar","categoryFieldKeys":["0"],"seriesFieldKeys":["0"],"aggregationType":"count","isStacked":false,"binsNumber":10,"wordFrequency":"-1","evaluatesOverAllRecords":false}}}}}},"spark_compute":{"compute_id":"/trident/default"},"dependencies":{"lakehouse":{"default_lakehouse":"75de1ae4-93f7-4582-9799-ca34d6429709","default_lakehouse_name":"Databard_Demo","default_lakehouse_workspace_id":"e35b5629-0c71-450d-b52f-4e2ecc6836c7"}}},"nbformat":4,"nbformat_minor":5}