{"cells":[{"cell_type":"markdown","source":["# Welcome to a PySpark notebook in Microsoft Fabric!\n","---\n","### This is an example of a *Markdown* cell\n","##### Before we look at some code, let's review some aspects of the UI\n","1. Attaching lakehouse(s) and warehouse(s) to the notebook (see below code snippet for environment info)\n","3. Setting the Default language for the notebook\n","4. Connect to Session\n","5. Run all button\n","\n","### Hey DataBard! Don't forget to tell them about Data Wrangler!"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"c898d845-9bff-406c-8b4e-1ae9f8ee3e29"},{"cell_type":"code","source":["#1. How do we access data in a connected lakehouse?\n","#Databard, do the demo everyone has to do with dimension_customer!\n","\n"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"},"collapsed":false,"jupyter":{"source_hidden":false}},"id":"f4c6f3e1-fb94-4bac-a8b0-c9efe2e7528f"},{"cell_type":"code","source":["#How do we write to storage?\n","#With the write function\n","#And since I have a lakehouse attached, I can access it relatively\n","df.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/dimension_customer_write\")\n","\n","#Without the lakehouse attached, here's the same code\n","#Note: If you run this in your environment, you'll have to copy the path in here.\n","df.write.mode(\"overwrite\").format(\"delta\").save(\"abfss://84e6d815-34b7-49bb-a433-ebec208e5cdb@onelake.dfs.fabric.microsoft.com/b389d6fd-e091-479b-ac43-6640a58407bd/Tables/dimension_customer_write\")\n","\n","#I can write the DataFrame to different formats.\n","df.write.mode(\"overwrite\").format(\"csv\").save(\"Files/CustomerCSVs/dimension_customer_write\")\n"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"ba8b27b0-6b0b-48f2-a496-c1c6335768e7"},{"cell_type":"code","source":["#Let's check with a data file I've prepared that contains data about artists in the music industry.\n","df = spark.read.format(\"csv\").option(\"header\",\"true\").load(\"Files/Input/updated_music_industry_data.csv\")\n","# df now is a Spark DataFrame containing CSV data\n","\n","#Display allows us to view the contents of a dataframe, as well as create charts.\n","display(df)"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"},"collapsed":false},"id":"398ce59e-9173-42d0-b3b8-70e318bdeef0"},{"cell_type":"markdown","source":["# What to do when Weird Al has taken over your data?!\n","# Data Wrangler to the rescue!\n","## Let's try the following:\n","### 1. Get rid of duplicates\n","### 2. Convert Age into an Integer\n","### 3. If time allows, split Name into first and last"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"3b873671-9e46-4964-be42-e9886fb5084a"},{"cell_type":"code","source":["#What if we want to access this notebook externally and make it dynamic?\n","#Parameter cell - These variables can be populated by external solutions, like pipelines.\n","\n","Parameter1 = ''\n","Parameter2 = 2"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":12,"statement_ids":[12],"state":"finished","livy_statement_state":"available","session_id":"0b32b888-659c-49d6-8c27-e47d949d5074","normalized_state":"finished","queued_time":"2025-01-16T16:22:23.0039739Z","session_start_time":null,"execution_start_time":"2025-01-16T16:22:29.0991686Z","execution_finish_time":"2025-01-16T16:22:29.3574907Z","parent_msg_id":"83fb1cf8-1852-435a-92b8-6fb942dd8c06"},"text/plain":"StatementMeta(, 0b32b888-659c-49d6-8c27-e47d949d5074, 12, Finished, Available, Finished)"},"metadata":{}}],"execution_count":10,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"},"tags":["parameters"]},"id":"e6686888-97d5-4971-8962-32d5a3d6fb7b"},{"cell_type":"code","source":["#What about all of these storage solutions I want to interact with? \n","#How do I reach them without using the UI?\n","import sempy\n","import sempy.fabric as fabric\n","import json\n","from pyspark.sql.functions import col,lit\n","import pandas as pd\n","\n","#Can I get the lakehouse ID for a lakehouse by finding its name?\n","lakehouseName = 'Databard_Demo'\n","\n","#Get basic details from the fabric library\n","workspaceID = fabric.get_workspace_id()\n","workspaceName = fabric.resolve_workspace_name(workspaceID)\n","\n","#Call the Fabric REST API, specifically looking at the lakehouse API\n","client = fabric.FabricRestClient()\n","response = client.get(f\"/v1/workspaces/{workspaceID}/lakehouses\")\n","responseJson = response.json()\n","items= pd.json_normalize(responseJson['value'], sep='_')\n","\n","#Create a dataframe and start looking for our values\n","df = spark.createDataFrame(items)\n","#display(df)\n","\n","#This is for context, showing some of the various properties we can get about lakehouses\n","result_df = df.select(lit(workspaceName).alias(\"WorkSpace\"),\n","                      col(\"id\").alias(\"LakehouseId\"),\n","                      col(\"displayName\").alias(\"Name\"),\n","                      col(\"type\").alias(\"Type\"),\n","                      col(\"description\").alias(\"Description\"),\n","                      col(\"properties_sqlEndpointProperties_connectionString\").alias(\"ConnectionString\"),\n","                      col(\"properties_oneLakeTablesPath\").alias(\"OneLakeTablePath\"),\n","                      col(\"properties_oneLakeFilesPath\").alias(\"OneLakeFilePath\"))\n","\n","#Filter to the record we want\n","result_df = result_df.filter(result_df[\"Name\"] == lakehouseName)\n","display(result_df)\n","\n","#Return Lakehouse ID of the desired lakehouse\n","result_lakehouseid = result_df.select(col(\"LakehouseId\"))\n","#display(result_lakehouseid)"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":17,"statement_ids":[17],"state":"finished","livy_statement_state":"available","session_id":"876796b9-07ac-4117-b80c-63ffaf63cd16","normalized_state":"finished","queued_time":"2025-03-27T16:10:42.9338797Z","session_start_time":null,"execution_start_time":"2025-03-27T16:10:42.9350107Z","execution_finish_time":"2025-03-27T16:10:44.4144891Z","parent_msg_id":"6f32f948-05f2-4de4-8e1e-d708d6a4ec98"},"text/plain":"StatementMeta(, 876796b9-07ac-4117-b80c-63ffaf63cd16, 17, Finished, Available, Finished)"},"metadata":{}},{"output_type":"display_data","data":{"application/vnd.synapse.widget-view+json":{"widget_id":"c138f059-ce42-42c9-96ed-33e01a4f022d","widget_type":"Synapse.DataFrame"},"text/plain":"SynapseWidget(Synapse.DataFrame, c138f059-ce42-42c9-96ed-33e01a4f022d)"},"metadata":{}}],"execution_count":15,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"},"collapsed":false},"id":"747c15c8-911c-4ecd-9102-37eb222a801b"},{"cell_type":"code","source":["#NotebookUtils Demo\n","#Notice how you don't have to manually create the environment and connection objects into the directory.\n","#Fabric already knows your environment. NotebookUtils gives you easy tools to access things.\n","import notebookutils\n","\n","#input parameters\n","base_directory_absolute = 'abfss://e416ff5b-6376-4614-a9a0-3f79fe99c1b0@onelake.dfs.fabric.microsoft.com/c1d01625-3177-4082-a112-4339acf9c69d/Files/Input'\n","base_directory = 'Files' #Simpler, right?\n","ignore_file_mask = 'Test'\n","file_target = 'music_industry_data.csv'\n","input_directory = f'{base_directory}/Input'\n","processed_directory = f'{base_directory}/Processed'\n","ignored_paths = {\"Input\", \"Processed\"}\n","\n","#Folder Creation and File Move functions\n","def create_base_architecture(base_directory : str):\n","    base_list = notebookutils.fs.ls(base_directory)\n","    #Remove ignored paths that may exist\n","    base_list = [file for file in base_list if file.name not in ignored_paths]\n","    #Create directories in case they don't exist\n","    notebookutils.fs.mkdirs(input_directory)\n","    notebookutils.fs.mkdirs(processed_directory)\n","    display(\"Processing directories created. Checking for files...\")\n","    if base_list:\n","        display(\"Files exist. Begin process...\")\n","        display(base_list)\n","        #assume all files in the base directory are input files\n","        for file in base_list:\n","            notebookutils.fs.mv(file.path, f\"{input_directory}/{file.name}\")     \n","    else:\n","        display(\"No files moved to initial directory. No files exist.\")\n","\n","def move_files(input_directory : str, processed_directory : str, target_file=None):\n","    file_list = notebookutils.fs.ls(input_directory)\n","    if file_list:\n","        #display(file_list)\n","        #Remove any files that don't contain the file mask\n","        #removed_list = [file for file in file_list if ignore_file_mask not in file.name]\n","        #display(\"The following files will be ignored. Please make sure they contain the file mask in order to be processed.\")\n","        #display(removed_list)\n","        file_list = [file for file in file_list if ignore_file_mask not in file.name]\n","\n","        for file in file_list:\n","            if (file.name == target_file) or (not target_file):\n","                display(f\"Current file: {file.name}\")\n","                notebookutils.fs.mv(file.path, f\"{processed_directory}/{file.name}\")\n","            else:\n","                display(f\"Current File {file.name} ignored. Not target file.\")\n","    else:\n","        display(\"No files moved. No files found in source directory.\")\n","\n","#Main section\n","create_base_architecture(base_directory=base_directory)\n","\n","move_files(input_directory, processed_directory)"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":39,"statement_ids":[39],"state":"finished","livy_statement_state":"available","session_id":"0e85067b-1873-4693-9ac7-b82aeff6f9d2","normalized_state":"finished","queued_time":"2025-03-27T18:52:21.4856495Z","session_start_time":null,"execution_start_time":"2025-03-27T18:52:21.4870393Z","execution_finish_time":"2025-03-27T18:52:23.9261267Z","parent_msg_id":"b70b97bc-74b9-4f98-8ee5-5b9548a74aa8"},"text/plain":"StatementMeta(, 0e85067b-1873-4693-9ac7-b82aeff6f9d2, 39, Finished, Available, Finished)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'Processing directories created. Checking for files...'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'No files moved to initial directory. No files exist.'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"'Current file: updated_music_industry_data.csv'"},"metadata":{}}],"execution_count":6,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"d15015b7-9144-44d8-af43-b88b2996082b"},{"cell_type":"markdown","source":["# Demo End"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"78be5669-937a-47e6-94e6-3510b76fef4d"},{"cell_type":"markdown","source":["# Debug Section Start"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"fdfead21-43bd-4134-a1ff-e1403d088fa1"},{"cell_type":"code","source":["##How did I create the Weird Al Data?\n","from pyspark.sql.functions import lit, when, col\n","import random\n","\n","# Demo data. Let's create a bunch of possible values for every column\n","names = [\"John Doe\", \"Jane Smith\", \"Alice Johnson\", \"Bob Brown\", \"Charlie Davis\", \"Diana Evans\", \"Eve Foster\", \"Frank Green\", \"Grace Harris\"]\n","first_names = [\"John\", \"Jane\", \"Alice\", \"Bob\", \"Charlie\", \"Diana\", \"Eve\", \"Frank\", \"Grace\", \"Smith\", \"Bill\", \"Jon\", \"Vivian\", \"Stacy\", \"Heidi\", \"Karen\", \"Otto\", \"Belinda\"]\n","last_names = [\"Doe\", \"Smith\", \"Johnson\", \"Brown\", \"Davis\", \"Evans\", \"Foster\", \"Green\", \"Harris\", \"Johann\", \"Pingel\", \"Kelbert\", \"Hiddleston\", \"Windsor\", \"Workmann\", \"Drews\"]\n","genres = [\"Rock\", \"Pop\", \"Jazz\", \"Classical\", \"Hip Hop\", \"Country\", \"Electronic\", \"Reggae\", \"Blues\", \"Metal\"]\n","addresses = [\"123 Main St\", \"456 Elm St\", \"789 Oak St\", \"101 Maple Ave\", \"202 Pine Rd\", \"303 Cedar Blvd\", \"404 Birch Ln\", \"505 Spruce Dr\", \"606 Willow Ct\", \"707 Aspen Pl\"]\n","emails = [\"example1@example.com\", \"example2@example.com\", \"example3@example.com\", \"example4@example.com\", \"example5@example.com\", \"example6@example.com\", \"example7@example.com\", \"example8@example.com\", \"example9@example.com\", \"example10@example.com\"]\n","phone_numbers = [\"555-1234\", \"555-5678\", \"555-8765\", \"555-4321\", \"555-6789\", \"555-9876\", \"555-3456\", \"555-6543\", \"555-7890\", \"555-0987\"]\n","\n","# Add in some environment info for where we're going to save our results\n","lakehouse_address = 'abfss://84e6d815-34b7-49bb-a433-ebec208e5cdb@onelake.dfs.fabric.microsoft.com/b389d6fd-e091-479b-ac43-6640a58407bd'\n","file_name = 'music_industry_data.csv'\n","updated_file_name = 'updated_music_industry_data.csv'\n","\n","#Create some derived variables based on the environment info\n","file_address = f\"{lakehouse_address}/Files/Input/{file_name}\"\n","updated_file_address = f\"{lakehouse_address}/Files/Input/{updated_file_name}\"\n","\n","# Create a function to generate random age as a string (whole number or decimal)\n","def generate_random_age():\n","    return random.randint(18, 70) if random.choice([True, False]) else random.uniform(18, 70)\n","\n","# Generate 100s of records in a Spark dataframe\n","data = spark.range(0, 100000000).withColumn(\"Name\", when((col(\"id\") < 20) | (col(\"id\") % 5 == 0), lit(\"Weird Al Yankovic\"))\n","                                            .otherwise(lit(random.choice(names)))) \\\n","                                .withColumn(\"FirstName\", when((col(\"id\") < 20) | (col(\"id\") % 5 == 0), lit(\"Weird Al\"))\n","                                            .otherwise(lit(random.choice(first_names)))) \\\n","                                .withColumn(\"LastName\", when((col(\"id\") < 20) | (col(\"id\") % 5 == 0), lit(\"Yankovic\"))\n","                                            .otherwise(lit(random.choice(last_names)))) \\\n","                                .withColumn(\"Age\", lit(generate_random_age())) \\\n","                                .withColumn(\"Address\", lit(random.choice(addresses))) \\\n","                                .withColumn(\"Email\", lit(random.choice(emails))) \\\n","                                .withColumn(\"PhoneNumber\", lit(random.choice(phone_numbers))) \\\n","                                .withColumn(\"Genre\", lit(random.choice(genres))) \\\n","                                .withColumn(\"AgeBucket\", lit(\"Unknown\"))\n","\n","\n","#We don't want the ID anymore, so we'll remove that here\n","data = data.drop(\"id\")\n","\n","\n","# Display the DataFrame\n","#print(data)\n","\n","# Save DataFrame to a CSV file\n","data.write.mode(\"overwrite\").format(\"csv\").option(\"header\", \"true\").save(file_address)\n","\n","# Now, let's mess with the data\n","# Obviously this isn't something you'd do in a production setting, this is purely for dramatic effect.\n","# Find the first record with the name 'Weird Al Yankovic'\n","first_weird_al_record = data.filter(col(\"Name\") == \"Weird Al Yankovic\").limit(1).collect()[0]\n","\n","data_updated = data.withColumn(\"Age\", when(col(\"Name\") == \"Weird Al Yankovic\", lit(first_weird_al_record[\"Age\"])).otherwise(col(\"Age\"))) \\\n","                   .withColumn(\"Address\", when(col(\"Name\") == \"Weird Al Yankovic\", lit(first_weird_al_record[\"Address\"])).otherwise(col(\"Address\"))) \\\n","                   .withColumn(\"Email\", when(col(\"Name\") == \"Weird Al Yankovic\", lit(first_weird_al_record[\"Email\"])).otherwise(col(\"Email\"))) \\\n","                   .withColumn(\"PhoneNumber\", when(col(\"Name\") == \"Weird Al Yankovic\", lit(first_weird_al_record[\"PhoneNumber\"])).otherwise(col(\"PhoneNumber\"))) \\\n","                   .withColumn(\"Genre\", when(col(\"Name\") == \"Weird Al Yankovic\", lit(first_weird_al_record[\"Genre\"])).otherwise(col(\"Genre\"))) \\\n","                   .withColumn(\"AgeBucket\", when(col(\"Name\") == \"Weird Al Yankovic\", lit(first_weird_al_record[\"AgeBucket\"])).otherwise(col(\"AgeBucket\")))\n","\n","\n","# # Display the updated DataFrame\n","# print(data_updated)\n","\n","# Save the updated DataFrame to a new CSV file\n","#df.to_csv(updated_file_address, index=False)\n","data_updated.write.format(\"csv\").mode(\"overwrite\").option(\"header\", \"true\").save(updated_file_address)"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":43,"statement_ids":[43],"state":"finished","livy_statement_state":"available","session_id":"03b8a06b-cf2f-4ab9-b99c-bf707df2f3d8","normalized_state":"finished","queued_time":"2025-04-01T14:35:40.6226148Z","session_start_time":null,"execution_start_time":"2025-04-01T14:35:40.6241015Z","execution_finish_time":"2025-04-01T14:36:11.6227095Z","parent_msg_id":"d3c2387f-4cca-4122-8bff-4fe5c2b5a387"},"text/plain":"StatementMeta(, 03b8a06b-cf2f-4ab9-b99c-bf707df2f3d8, 43, Finished, Available, Finished)"},"metadata":{}}],"execution_count":6,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"},"editable":true,"run_control":{"frozen":false}},"id":"f125171a-39ee-49b0-9627-2b6b154aa01f"},{"cell_type":"code","source":["#Rollback move demo\n","move_files(processed_directory, input_directory)\n"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":14,"statement_ids":[14],"state":"finished","livy_statement_state":"available","session_id":"876796b9-07ac-4117-b80c-63ffaf63cd16","normalized_state":"finished","queued_time":"2025-03-27T15:55:47.0692297Z","session_start_time":null,"execution_start_time":"2025-03-27T15:55:47.0704363Z","execution_finish_time":"2025-03-27T15:55:47.8810056Z","parent_msg_id":"ed4a8999-89e5-4c3c-babb-7d787ddf64f3"},"text/plain":"StatementMeta(, 876796b9-07ac-4117-b80c-63ffaf63cd16, 14, Finished, Available, Finished)"},"metadata":{}},{"output_type":"execute_result","execution_count":41,"data":{"text/plain":"True"},"metadata":{}}],"execution_count":12,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"335119de-41f7-4a3d-83aa-2a86648e0e37"},{"cell_type":"code","source":["# Demo 2\n","# We know our enviroment. Let's Transform Data!\n","\n","from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n","from pyspark.sql.functions import lit, when\n","import pandas as pd\n","import resource #Used for assessing resources used\n","import random\n","from pyspark.context import SparkContext\n","\n","# # Function to generate data for the dataframe\n","def generate_data(num_rows):\n","    names = ['Alice', 'Bob', 'Charlie']\n","    ages = [25, 30, 35]\n","    occupations = ['Engineer', 'Analyst', 'Manager']\n","    \n","    data = {'Name': ['Alice'], 'Age': [25], 'Occupation': ['Engineer']}  # Fix: Change 'FirstName' to 'Name'\n","    \n","    for _ in range(num_rows):\n","        data['Name'].append(random.choice(names))  # Fix: Change 'FirstName' to 'Name'\n","        data['Age'].append(random.choice(ages))\n","        data['Occupation'].append(random.choice(occupations))\n","    \n","    return spark.createDataFrame(data, ['Name', 'Age', 'Occupation'])\n","\n","\n","# Example: Create a simple dataframe with sample data\n","# Here, we create a DataFrame named df using the createDataFrame method provided by the SparkSession. \n","# The DataFrame is a distributed collection of data organized into named columns. \n","# In this case, we pass a list of tuples containing the sample data and specify the column names as 'Name' and 'Age'.\n","data = [('Alice', 25, 'Manager'), ('Bob', 30, 'Analytst'), ('Charlie', 35, 'Engineer')]\n","df = spark.createDataFrame(data, ['Name', 'Age', 'Occupation'])\n","\n","# new_data = generate_data(100000)\n","# df = df.union(new_data)\n","\n","# Print the initial dataframe to the console in a tabular format\n","print(\"Initial DataFrame:\")\n","df.show()\n","\n","# Display the column names and data types\n","print(\"Data Types:\")\n","df.printSchema()\n","\n","# Step 1: Profile the data - Calculate the average age\n","# The selectExpr() method is used to select and compute an expression on the DataFrame. \n","# In this case, we calculate the average of the 'Age' column and alias it as 'avg_age'. \n","# The collect() method is then used to retrieve the result as a list, and we access the average age value using indexing.\n","average_age = df.selectExpr('avg(Age) as avg_age').collect()[0]['avg_age']\n","print(\"Average Age:\", average_age)\n","\n","# Step 2: Transform the data - Add a new column 'Category' using a case statement\n","# In this step, we add another new column named 'Category' to the DataFrame using a case statement. \n","# The when() function is used to define the conditions and corresponding values for the 'Category' column. \n","# If the 'Age' is less than 30, the value is set to 'Young'. \n","# If the 'Age' is greater than or equal to 30, the value is set to 'Adult'.\n","# Otherwise, the value is set to 'Unknown'. Finally, we print the DataFrame with the new column.\n","df = df.withColumn('Category', when(df['Age'] < 30, 'Young')\n","                               .when(df['Age'] >= 30, 'Adult')\n","                               .otherwise('Unknown'))\n","print(\"\\nDataFrame with Category column:\")\n","df.show()\n","\n","# Step 3: Profile the data - Count the number of records by occupation\n","#Here, we perform a profiling operation on the DataFrame by counting the number of records for each occupation. \n","# The groupBy() method is used to group the DataFrame by the 'Occupation' column, and the count() method is applied to calculate the count for each group. \n","# The result is stored in the occupation_counts DataFrame, and we print it to display the occupation counts.\n","occupation_counts = df.groupBy('Occupation').count()\n","print(\"\\nOccupation Counts:\")\n","occupation_counts.show()\n","\n","# Step 4: Transform the data - Filter records based on age\n","# In this step, we filter the DataFrame to include only the records where the age is greater than 30. \n","# The filter() method is used to apply the filtering condition, and the resulting DataFrame is stored in filtered_df. \n","filtered_df = df.filter(df['Age'] > 30)\n","print(\"\\nFiltered DataFrame:\")\n","filtered_df.show()\n","\n","# Step 5: Profile the data - Describe the dataframe\n","# The describe() method computes summary statistics for each numerical column in the DataFrame, including count, mean, standard deviation, minimum, and maximum values. \n","# The show() method is then used to display the summary statistics, including count, mean, standard deviation, minimum, and maximum values.\n","print(\"\\nDataFrame Description:\")\n","df.describe().show()\n","\n","# Step 6: Display the dataframe\n","# Finally, we display the DataFrame with the new columns and transformations applied.\n","# The display() function is a utility provided by PySpark to render the DataFrame in a way that is suitable for Jupyter notebooks or other frontends. \n","# It allows for more advanced visualizations and interactivity compared to the regular show() method.\n","display(df)\n","\n","# Bonus Content: Create a second dataframe with explicit schema\n","# In this part, we create a second DataFrame named df_explicit_schema with an explicit schema. \n","# The schema is defined using the StructType and StructField classes from the pyspark.sql.types module. \n","# The schema specifies the column names, data types, and nullability constraints. \n","# We then create the DataFrame using the createDataFrame() method, passing the sample data and the explicit schema. Finally, we print the second DataFrame with the explicit schema.\n","schema = StructType([\n","    StructField('Name', StringType(), nullable=False),\n","    StructField('Age', IntegerType(), nullable=False),\n","    StructField('Occupation', StringType(), nullable=False)\n","])\n","df_explicit_schema = spark.createDataFrame(data, schema)\n","\n","print(\"\\nSecond DataFrame with Explicit Schema:\")\n","df_explicit_schema.show()\n","\n","# Define the target directory for the Delta table\n","target_directory = \"\"\n","\n","# Save the DataFrame to the Delta table with overwrite mode\n","df.write.format(\"delta\").mode(\"overwrite\").save(target_directory)\n","\n"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"},"collapsed":false},"id":"18aa8ce8-cdbd-4a72-86e9-0050489b6c71"},{"cell_type":"code","source":["#What if I want details about my environment?\n","\n","#From Bradley Schacht, 'Gathering useful notebook and environment details at runtime'\n","\n","#import trident\n","\n","default_lakehouse_id    = 'No default lakehouse' if spark.conf.get(\"trident.lakehouse.id\") == '' else spark.conf.get(\"trident.lakehouse.id\")\n","default_lakehouse_name  = 'No default lakehouse' if spark.conf.get(\"trident.lakehouse.name\") == '' else spark.conf.get(\"trident.lakehouse.name\")\n","notebook_item_id        = spark.conf.get(\"trident.artifact.id\")\n","#notebook_item_name      = spark.conf.get(\"trident.artifact.name\")\n","pool_executor_cores     = spark.sparkContext.getConf().get(\"spark.executor.cores\")\n","pool_executor_memory    = spark.sparkContext.getConf().get(\"spark.executor.memory\")\n","pool_min_executors      = spark.sparkContext.getConf().get(\"spark.dynamicAllocation.minExecutors\")\n","pool_max_executors      = spark.sparkContext.getConf().get(\"spark.dynamicAllocation.maxExecutors\")\n","pool_number_of_nodes    = len(str(sc._jsc.sc().getExecutorMemoryStatus().keys()).replace(\"Set(\",\"\").replace(\")\",\"\").split(\", \"))\n","spark_app_name          = spark.sparkContext.getConf().get(\"spark.app.name\")[::-1].split(\"_\",1)[0][::-1]\n","workspace_id            = spark.conf.get(\"trident.artifact.workspace.id\")\n","#workspace_name          = spark.conf.get(\"trident.artifact.workspace.name\")\n","\n","print(f'default_lakehouse_id:   {default_lakehouse_id}')\n","print(f'default_lakehouse_name: {default_lakehouse_name}')\n","print(f'notebook_item_id:       {notebook_item_id}')\n","#print(f'notebook_item_name:     {notebook_item_name}')\n","print(f'spark_app_name:         {spark_app_name}')\n","print(f'pool_executor_cores:    {pool_executor_cores}')\n","print(f'pool_executor_memory:   {pool_executor_memory}')\n","print(f'pool_min_executors:     {pool_min_executors}')\n","print(f'pool_max_executors:     {pool_max_executors}')\n","print(f'pool_number_of_nodes:   {pool_number_of_nodes}')\n","print(f'workspace_id:           {workspace_id}')\n","#print(f'workspace_name:         {workspace_name}')\n","\n","#Did you attach a default lakehouse?\n","\n","#What about accessing Lakehouses outside of the default? Copy Path!\n"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":13,"statement_ids":[13],"state":"finished","livy_statement_state":"available","session_id":"0b32b888-659c-49d6-8c27-e47d949d5074","normalized_state":"finished","queued_time":"2025-01-16T16:22:23.0818619Z","session_start_time":null,"execution_start_time":"2025-01-16T16:22:29.8093976Z","execution_finish_time":"2025-01-16T16:22:30.6448564Z","parent_msg_id":"33f6de40-0c5d-4401-a91f-e035019c12ff"},"text/plain":"StatementMeta(, 0b32b888-659c-49d6-8c27-e47d949d5074, 13, Finished, Available, Finished)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["default_lakehouse_id:   75de1ae4-93f7-4582-9799-ca34d6429709\ndefault_lakehouse_name: Databard_Demo\nnotebook_item_id:       2dd1eb8c-34cd-491f-9575-850a064f397f\nspark_app_name:         0b32b888-659c-49d6-8c27-e47d949d5074\npool_executor_cores:    8\npool_executor_memory:   56g\npool_min_executors:     1\npool_max_executors:     1\npool_number_of_nodes:   2\nworkspace_id:           e35b5629-0c71-450d-b52f-4e2ecc6836c7\n"]}],"execution_count":11,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"f3d855a3-3504-4ba6-a02f-69808ee2a829"},{"cell_type":"code","source":["%%pyspark \n","!echo \"spark.trident.pbiApiVersion=v1\">>/home/trusted-service-user/.trident-context\n","#Word to the wise! If you are using Fabric Lakehouse schemas in preview, there is a known bug that returns a 'forbidden' error when interacting with the lakehouse.\n","#If you run into this, use this code to work around it!\n","#This should only be an issue until schemas go GA"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":9,"statement_ids":[9],"state":"finished","livy_statement_state":"available","session_id":"0b32b888-659c-49d6-8c27-e47d949d5074","normalized_state":"finished","queued_time":"2025-01-16T16:22:22.7684621Z","session_start_time":null,"execution_start_time":"2025-01-16T16:22:24.0925598Z","execution_finish_time":"2025-01-16T16:22:24.4379392Z","parent_msg_id":"ea4c92ab-a3e5-48a8-9085-fc63b9a88540"},"text/plain":"StatementMeta(, 0b32b888-659c-49d6-8c27-e47d949d5074, 9, Finished, Available, Finished)"},"metadata":{}}],"execution_count":7,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"4108c2f8-7ff4-4e82-8af3-3e289164ccc5"},{"cell_type":"markdown","source":["## Tips for Formatting Text in Markdown\n","\n","Markdown provides several options for formatting text in a Jupyter Notebook markdown cell. These options include:\n","\n","### 1. Headers\n","\n","Headers are used to create different levels of headings. There are six levels of headers available in markdown, denoted by the number of hash symbols (#) used before the text. For example:\n","\n","```\n","# Heading 1\n","## Heading 2\n","### Heading 3\n","#### Heading 4\n","##### Heading 5\n","###### Heading 6\n","```\n","\n","### 2. Emphasis\n","\n","To emphasize text, you can use asterisks (*) or underscores (_) around the text. Here are some examples:\n","\n","```\n","*Italic text*\n","_Italic text_\n","\n","**Bold text**\n","__Bold text__\n","\n","***Bold and italic text***\n","___Bold and italic text___\n","```\n","\n","### 3. Lists\n","\n","Markdown supports both ordered and unordered lists. For unordered lists, you can use asterisks (*), plus signs (+), or hyphens (-) as bullet points. For example:\n","\n","```\n","- Item 1\n","- Item 2\n","- Item 3\n","\n","* Item 1\n","* Item 2\n","* Item 3\n","\n","+ Item 1\n","+ Item 2\n","+ Item 3\n","```\n","\n","For ordered lists, you can use numbers followed by periods. For example:\n","\n","```\n","1. Item 1\n","2. Item 2\n","3. Item 3\n","```\n","\n","### 4. Links\n","\n","To create a hyperlink, you can use square brackets [] to enclose the link text, followed by parentheses () containing the URL. For example:\n","\n","```\n","[GitHub](https://github.com)\n","```\n","\n","### 5. Images\n","\n","To display an image, you can use an exclamation mark (!), followed by square brackets [] containing the alt text, and parentheses () containing the image URL. For example:\n","\n","```\n","![Alt Text](https://example.com/image.jpg)\n","```\n","\n","### 6. Code Blocks\n","\n","To display code blocks, you can use triple backticks (```) before and after the code. You can also specify the programming language for syntax highlighting. For example:\n","\n","\\```python\n","print(\"Hello, World!\")\n","\\```\n","\n","### 7. Horizontal Lines\n","\n","To insert a horizontal line, you can use three or more hyphens (-), asterisks (*), or underscores (_). For example:\n","\n","```\n","---\n","```\n","\n","These are just some of the formatting options available in markdown. You can explore more advanced features and syntax by referring to the markdown documentation.\n","\n","Feel free to experiment with these formatting options in your Jupyter Notebook markdown cells to create visually appealing and well-structured documentation for your workflow.\n"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"415ff63e-1192-4dba-8db4-c9fe8924687d"}],"metadata":{"kernel_info":{"name":"synapse_pyspark"},"kernelspec":{"name":"synapse_pyspark","language":"Python","display_name":"Synapse PySpark"},"language_info":{"name":"python"},"microsoft":{"language":"python","language_group":"synapse_pyspark","ms_spell_check":{"ms_spell_check_language":"en"}},"widgets":{},"nteract":{"version":"nteract-front-end@1.0.0"},"synapse_widget":{"version":"0.1","state":{"c138f059-ce42-42c9-96ed-33e01a4f022d":{"type":"Synapse.DataFrame","sync_state":{"table":{"rows":[{"0":"ABTS - Kuehn","1":"c1d01625-3177-4082-a112-4339acf9c69d","2":"Databard_Demo","3":"Lakehouse","4":"","5":"x5rzgyoylndubmeulb65omttsi-lp7rnzdwmmkenknah5475gobwa.datawarehouse.fabric.microsoft.com","6":"https://onelake.dfs.fabric.microsoft.com/e416ff5b-6376-4614-a9a0-3f79fe99c1b0/c1d01625-3177-4082-a112-4339acf9c69d/Tables","7":"https://onelake.dfs.fabric.microsoft.com/e416ff5b-6376-4614-a9a0-3f79fe99c1b0/c1d01625-3177-4082-a112-4339acf9c69d/Files","index":0,"key":0}],"schema":[{"key":"0","name":"WorkSpace","type":"string"},{"key":"1","name":"LakehouseId","type":"string"},{"key":"2","name":"Name","type":"string"},{"key":"3","name":"Type","type":"string"},{"key":"4","name":"Description","type":"string"},{"key":"5","name":"ConnectionString","type":"string"},{"key":"6","name":"OneLakeTablePath","type":"string"},{"key":"7","name":"OneLakeFilePath","type":"string"}],"truncated":false},"isSummary":false,"language":"scala","wranglerEntryContext":{"candidateVariableNames":["result_df"],"dataframeType":"pyspark"}},"persist_state":{"view":{"type":"details","tableOptions":{},"chartOptions":{"chartType":"bar","categoryFieldKeys":["0"],"seriesFieldKeys":["2"],"aggregationType":"count","isStacked":false,"binsNumber":10,"wordFrequency":"-1","evaluatesOverAllRecords":false},"viewOptionsGroup":[{"tabItems":[{"type":"table","name":"Table","key":"0","options":{}}]}]}}}}},"spark_compute":{"compute_id":"/trident/default","session_options":{"conf":{"spark.synapse.nbs.session.timeout":"1200000"}}},"dependencies":{"lakehouse":{"default_lakehouse":"b389d6fd-e091-479b-ac43-6640a58407bd","known_lakehouses":[{"id":"b389d6fd-e091-479b-ac43-6640a58407bd"}],"default_lakehouse_name":"LH_Databard_Demo","default_lakehouse_workspace_id":"84e6d815-34b7-49bb-a433-ebec208e5cdb"},"warehouse":{"known_warehouses":[{"id":"b85e98f0-ef72-4e36-901d-d6d346c39f2c","type":"Datawarehouse"},{"id":"d8ffa758-2c79-4f89-97da-c34420b2157f","type":"Lakewarehouse"}],"default_warehouse":"b85e98f0-ef72-4e36-901d-d6d346c39f2c"},"environment":{}}},"nbformat":4,"nbformat_minor":5}